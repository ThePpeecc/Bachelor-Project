{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import collections as matcoll\n",
    "from mpl_toolkits import mplot3d\n",
    "import pyLDAvis\n",
    "\n",
    "# Import numpy and pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Metrics\n",
    "from scipy.stats import pearsonr\n",
    "from tools import computeScore\n",
    "\n",
    "# Data Loader\n",
    "import loader\n",
    "\n",
    "# Import GSDMM algorithm model\n",
    "from gsdmm import MovieGroupProcess\n",
    "\n",
    "# Utilz\n",
    "import tools\n",
    "import math\n",
    "from tools import vocabCreater, getMoreSent, getMoreTokens, rawTokenize, cleanSent, cleanDoc, stemmedReverse, Id2Word\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = loader.LoadRaw()\n",
    "data = getMoreSent(raw)\n",
    "tokens = tools.tokenize(data)\n",
    "stemmed = tools.stemDocument(tokens)\n",
    "stemmed = getMoreTokens(stemmed)\n",
    "rawTokens = rawTokenize(data)\n",
    "seed = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocabCreater(tokens)\n",
    "stemmed_vocab = vocabCreater(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(vocab, n = 5, start = 0, fileName = 'NONE'):\n",
    "    \n",
    "    vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    labels = np.array(vocab)[start:start+n,0]\n",
    "    counts = np.array(vocab)[start:start+n,1].astype(int)\n",
    "\n",
    "    bar_width = 0.35\n",
    "\n",
    "    indexes = np.arange(len(labels))\n",
    "\n",
    "    fig = plt.figure(figsize=(20,10))\n",
    "\n",
    "    plt.bar(indexes, counts)\n",
    "\n",
    "    # add labels\n",
    "    plt.xticks(indexes + bar_width, labels)\n",
    "    plt.show()\n",
    "    if fileName != \"NONE\":\n",
    "        plt.draw()\n",
    "        fig.savefig(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = stemmedReverse(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = cleanDoc(stemmed, 3, 50)\n",
    "cleanedVocab = sorted(vocabCreater(cleaned).items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "len(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = Id2Word(cleaned)\n",
    "\n",
    "def score(dist, num = 5):\n",
    "    topics = [np.array(list(l.items()))[:num,0] if len(l) >= num else [] for l in dist]\n",
    "    return computeScore(list(filter(lambda x: len(x) > 0, topics)), cleaned, dictionary)\n",
    "\n",
    "def top_words(dis, count, index, num = 5, start = 0, minCount = 1):\n",
    "\n",
    "    for i in index:\n",
    "        if (count[i]) > minCount - 1:\n",
    "            s = sorted(dis[i].items(), key=lambda x: x[1], reverse=True)\n",
    "            print(\"Cluster {} with {} documents : {}\".format(i, count[i], s[start:start+num]))\n",
    "    \n",
    "    \n",
    "def formatTFIDFString(clust):\n",
    "\n",
    "    formatted = []\n",
    "    for word, value in clust:\n",
    "        formatted.append(\"{}: {:.1f}\".format(word, value))\n",
    "        \n",
    "    return formatted\n",
    "\n",
    "\n",
    "def tf_idf_creator(dis, index):\n",
    "    # First we build a dictionary to look up the count of the words in each cluster\n",
    "    cross_vocab = {}\n",
    "\n",
    "    for i, clus in enumerate(dis):\n",
    "        for word, count in clus.items():\n",
    "            if word not in cross_vocab:\n",
    "                cross_vocab[word] = np.zeros(len(index))\n",
    "                cross_vocab[word][i] += 1\n",
    "            else:\n",
    "                cross_vocab[word][i] += 1\n",
    "\n",
    "    rescored = [{} for _ in range(len(index))]\n",
    "    for i in index:\n",
    "        newDict = {}\n",
    "        for word, count in dis[i].items():\n",
    "            newDict[word] = count * math.log(len(index)/sum(cross_vocab[word]))\n",
    "        rescored[i] = newDict\n",
    "        \n",
    "    return rescored\n",
    "    \n",
    "    \n",
    "# Simple tf-idf sorter\n",
    "def top_tf_idf(dis, doc_count, index, num = 5, start = 0, minCount = 1):\n",
    "    \n",
    "    rescored = tf_idf_creator(dis, index)\n",
    "    for i in index:\n",
    "        if (doc_count[i]) > minCount - 1:\n",
    "            s = sorted(rescored[i].items(), key=lambda x: x[1], reverse=True)\n",
    "            print(\"Cluster {} with {} documents : {}\".format(i, doc_count[i], formatTFIDFString(s[start:start+num])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Experiment\n",
    "\n",
    "In the following code block there is a large amount of code dedicated to runnning a GSDMM experiment with certain hyperparameter and plotting the results of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotClusterScore(runs):\n",
    "    # Plot how the average run over all the runs cluster scores change. \n",
    "    # Here the score is the coherence score, and clusters means the number of populated clusters at the end of a training session\n",
    "    endClusters = list(map(lambda x: x['runValues'][2][-1], runs))\n",
    "    scores = list(map(lambda x: x['score'], runs))\n",
    "    \n",
    "    avgList = {}\n",
    "    \n",
    "    for c, s in zip(endClusters, scores):\n",
    "        if c in avgList:\n",
    "            avgList[c].append(s)\n",
    "        else:\n",
    "            avgList[c] = [s]\n",
    "    score2Cluster = []\n",
    "    for key, value in avgList.items():\n",
    "        avg = sum(value)/len(value)\n",
    "        score2Cluster.append((key, avg))\n",
    "    score2Cluster.sort(key=lambda x: x[0])\n",
    "    score2Cluster = np.array(score2Cluster)\n",
    "    print(\"Pearsons Correlation Score: {:.4f}\".format(pearsonr(score2Cluster[:,0], score2Cluster[:,1])[0]))\n",
    "    plt.plot(score2Cluster[:,0], score2Cluster[:,1])\n",
    "    plt.xlabel('Number of populated topics')\n",
    "    plt.ylabel('Average coherence score')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plotBetaScore(runs):\n",
    "    # Plot the average coherence score for beta values\n",
    "    beta = list(map(lambda x: round(x['beta'], 2), runs))\n",
    "    alpha = list(map(lambda x: round(x['alpha'], 2), runs))\n",
    "    scores = list(map(lambda x: x['score'], runs))\n",
    "    \n",
    "    avgList = {}\n",
    "    \n",
    "    for b, a, s in zip(beta, alpha, scores):\n",
    "        if b in avgList:\n",
    "            avgList[b].append(s)\n",
    "        else:\n",
    "            avgList[b] = [s]\n",
    "    \n",
    "\n",
    "    beta2Score = []\n",
    "    for key, value in avgList.items():\n",
    "        avg = sum(value)/len(value)\n",
    "        beta2Score.append((key, avg))\n",
    "\n",
    "    \n",
    "    beta2Score.sort(key=lambda x: x[0])\n",
    "    beta2Score = np.array(beta2Score)\n",
    "    print(\"Pearsons Correlation Score: {:.4f}\".format(pearsonr(beta2Score[:,0], beta2Score[:,1])[0]))\n",
    "    plt.plot(beta2Score[:,0], beta2Score[:,1])\n",
    "    plt.xlabel('Beta value')\n",
    "    plt.ylabel('Average coherence score')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotAlphaScore(runs):\n",
    "    # Plot the average coherence score for alpha values\n",
    "    beta = list(map(lambda x: round(x['beta'], 2), runs))\n",
    "    alpha = list(map(lambda x: round(x['alpha'], 2), runs))\n",
    "    scores = list(map(lambda x: x['score'], runs))\n",
    "    \n",
    "    avgList = {}\n",
    "    \n",
    "    for b, a, s in zip(beta, alpha, scores):\n",
    "        if a in avgList:\n",
    "            avgList[a].append(s)\n",
    "        else:\n",
    "            avgList[a] = [s]\n",
    "    \n",
    "\n",
    "    alpha2Score = []\n",
    "    for key, value in avgList.items():\n",
    "        avg = sum(value)/len(value)\n",
    "        alpha2Score.append((key, avg))\n",
    "\n",
    "\n",
    "    alpha2Score.sort(key=lambda x: x[0])\n",
    "    alpha2Score = np.array(alpha2Score)\n",
    "    print(\"Pearsons Correlation Score: {:.4f}\".format(pearsonr(alpha2Score[:,0], alpha2Score[:,1])[0]))\n",
    "    plt.plot(alpha2Score[:,0], alpha2Score[:,1])\n",
    "    plt.xlabel('Alpha value')\n",
    "    plt.ylabel('Average coherence score')\n",
    "    plt.show()\n",
    "      \n",
    "def plotAlphaBetaScore3D(runs):    \n",
    "    # Plot the coherence score of alpha and beta in 3D (not recommened, as it is hard to tell what is happening)\n",
    "    beta = list(map(lambda x: round(x['beta'], 2), runs))\n",
    "    alpha = list(map(lambda x: round(x['alpha'], 2), runs))\n",
    "    scores = list(map(lambda x: x['score'], runs))\n",
    "\n",
    "    xalpha = np.repeat(np.arange(0.1, 1.0, 0.1), 9).reshape((9,9)).T\n",
    "    ybeta = np.repeat(np.arange(0.1, 1.0, 0.1), 9).reshape((9,9))\n",
    "    \n",
    "    zscore = np.repeat(0.0, 9*9).reshape((9,9))\n",
    "    \n",
    "    for b, a, s in zip(beta, alpha, scores):\n",
    "        b = int(b*10)-1\n",
    "        a = int(a*10)-1\n",
    "        zscore[a][b] = s\n",
    "\n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes(projection='3d')\n",
    "    # Alternative plotting types\n",
    "    #ax.contour3D(xalpha, ybeta, zscore, 50, cmap='binary')\n",
    "    #ax.plot_surface(xalpha, ybeta, zscore, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "    ax.scatter(alpha, beta, scores, c=scores, cmap='viridis', linewidth=0.5);\n",
    "    ax.set_xlabel('alpha')\n",
    "    ax.set_ylabel('beta')\n",
    "    ax.set_zlabel('score')\n",
    "#    ax.view_init(60, 35)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def plotAlphaBetaScore2D(runs):\n",
    "    # Plot the coherence score of alpha and beta in 2D (recommened, as it is easier to see what is ahppening)\n",
    "    zscore = np.repeat(0.0, 9*9).reshape((9,9))\n",
    "    \n",
    "    for run in runs:\n",
    "        b = int(run['beta']*10)-1\n",
    "        a = int(run['alpha']*10)-1    \n",
    "        zscore[b][a] = run['score']\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    ax = plt.axes()\n",
    "    \n",
    "    #np.array(scores)*100\n",
    "    plt.imshow(zscore, cmap='viridis', origin='lower')\n",
    "    plt.colorbar()\n",
    "    \n",
    "    plt.xticks(range(9))\n",
    "    ax.set_xticklabels(map(lambda x: round(x, 2), np.arange(0.1, 1.0, 0.1)))\n",
    "    ax.set_yticklabels(map(lambda x: round(x, 2), np.arange(0.0, 1.0, 0.1)))\n",
    "\n",
    "    ax.set_xlabel('alpha')\n",
    "    ax.set_ylabel('beta')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plotTrainingSession(run):\n",
    "    # Plot a specific training run\n",
    "    _, transferred, clusters = run\n",
    "    \n",
    "    plt.figure(figsize = (10,5))\n",
    "    plt.plot(transferred)\n",
    "    plt.ylabel('Transferred Elements')\n",
    "    plt.xlabel('Stage')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize = (10,5))\n",
    "    plt.plot(clusters)\n",
    "    plt.ylabel('Populated Clusters')\n",
    "    plt.xlabel('Stage')\n",
    "    plt.show()\n",
    "\n",
    "def lineScatter(x, y, size = (20,5), base = 5, title = '', xLabel = '', yLabel = '', hLine = -1000, hlabel=''):\n",
    "    # Line scatter plot\n",
    "    lines = []\n",
    "    for i in range(len(x)):\n",
    "        pair=[(x[i],0), (x[i], y[i])]\n",
    "        lines.append(pair)\n",
    "        \n",
    "    linecoll = matcoll.LineCollection(lines)\n",
    "    fig, ax = plt.subplots(figsize = size)\n",
    "    ax.add_collection(linecoll)\n",
    "\n",
    "    plt.scatter(x,y)\n",
    "    plt.axhline(y=hLine, color='b', linestyle='-', label=hlabel)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.ylabel(yLabel)\n",
    "    plt.xlabel(xLabel)\n",
    "\n",
    "    plt.xticks(x)\n",
    "    plt.ylim(0,max(y) + min(y) + base)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def experiment(data, vocab, seed = 0, alpha = .1, beta = .1, step = .1, n_iters = 10, cluster_count = 10, topicWordScore = 10):\n",
    "    # Runs an experiment, and results in a dataset of different\n",
    "    # training runs that can be used to figure out the optimal hyper parameters\n",
    "    loopAlpha = np.arange(alpha, 1.0, step)\n",
    "    loopBeta = np.arange(beta, 1.0, step)\n",
    "    \n",
    "    runs = []\n",
    "    \n",
    "    vocab = set(np.array(vocab)[:,0])\n",
    "    n_terms = len(vocab)\n",
    "    \n",
    "    print(\"Running experiments\")\n",
    "    for a in loopAlpha:\n",
    "        for b in loopBeta:    \n",
    "            np.random.seed(seed)\n",
    "            print(\"alpha: {}, beta: {}\".format(a, b))\n",
    "                  \n",
    "            mgp = MovieGroupProcess(K=cluster_count, alpha=a, beta=b, n_iters=n_iters)\n",
    "            y = mgp.fit(data, n_terms, verbose=False)\n",
    "            \n",
    "            iterClass = { 'alpha': a, 'beta': b, 'score': score(mgp.cluster_word_distribution, topicWordScore), 'runValues': y}\n",
    "            print(\"score: {}\".format(iterClass['score']))\n",
    "            runs.append(iterClass)\n",
    "            \n",
    "    return runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runExperiment = False # Set to true to run experiment, WARNING, can take a long time depending on setup\n",
    "if runExperiment:\n",
    "    runs = experiment(cleaned, cleanedVocab, n_iters=50, cluster_count=50)    \n",
    "    plotClusterScore()\n",
    "    plotBetaScore()\n",
    "    plotAlphaScore()\n",
    "    plotAlphaBetaScore2D()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = 50\n",
    "np.random.seed(seed)\n",
    "mgp = MovieGroupProcess(K=cluster_count, alpha=.2, beta=.2, n_iters=50)\n",
    "vocab = set(np.array(cleanedVocab)[:,0])\n",
    "n_terms = len(vocab)\n",
    "y = mgp.fit(cleaned, n_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "doc_count = np.array(mgp.cluster_doc_count)\n",
    "mincount = 15\n",
    "topicWordScore = 10\n",
    "\n",
    "print(\"Number of valid topics: {}\".format(sum([count > mincount-1 for count in doc_count])))\n",
    "\n",
    "print('Number of documents per topic :', doc_count)\n",
    "print('*'*20)\n",
    "# Topics sorted by the number of document they are allocated to\n",
    "top_index = doc_count.argsort()[-cluster_count:][::-1]\n",
    "print('Most important clusters (by number of docs inside):', top_index)\n",
    "print('*'*20)\n",
    "# Show the top 5 words in term frequency for each cluster \n",
    "top_words(mgp.cluster_word_distribution, doc_count, top_index, 5, minCount = mincount)\n",
    "print('*'*20)\n",
    "print('TF-IDF sorted')\n",
    "print('*'*20)\n",
    "top_tf_idf(mgp.cluster_word_distribution, doc_count, top_index, 5, minCount = mincount)\n",
    "print(\"Coherence Score: \", score(mgp.cluster_word_distribution, topicWordScore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineScatter(range(len(doc_count)), doc_count, xLabel = 'Topics', yLabel = 'Number of Documents in Topic', size = (15,5), hLine = mincount, hlabel=\"Minimum amount to Pass as a topic\")\n",
    "plotTrainingSession(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup['rund'] # Lookup words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_tabel(data, model):\n",
    "    # Creates a table for \n",
    "    tokens = tools.tokenize(data)\n",
    "    stemmed = tools.stemDocument(tokens)\n",
    "    stemmed = getMoreTokens(stemmed)\n",
    "    stemmed_vocab = vocabCreater(stemmed)\n",
    "    \n",
    "    newAr = []    \n",
    "    vocab_count = {l: v for l, v in stemmed_vocab.items()} \n",
    "\n",
    "\n",
    "    for i, sent in enumerate(stemmed):\n",
    "        nSent = cleanSent(sent, vocab_count)\n",
    "        label, prob = model.choose_best_label(nSent)\n",
    "        newAr.append([data[i], nSent, label, prob])\n",
    "            \n",
    "    df = pd.DataFrame(newAr)\n",
    "    df.columns =[\"Sentence\", \"Cleaned\", \"Cluster\", \"Probability\"]\n",
    "    return df\n",
    "\n",
    "def lookupCluster(mgp, clus = 0, num = 100):\n",
    "    # Lookup the sentences in a certain cluster\n",
    "    pd.set_option('display.max_colwidth', None)\n",
    "    pd.options.display.max_rows = 100\n",
    "    df = doc_tabel(data, mgp).sort_values(by=['Probability'], ascending=False)\n",
    "    return df.loc[df['Cluster'] == clus].head(num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lookupCluster(mgp, clus = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def ClusterBarCharts(mgp, top_index, clust = 0):\n",
    "    # Creates and displays bar charts over most frequent words and most highscoring tf-idf words\n",
    "    rescored = tf_idf_creator(mgp.cluster_word_distribution, top_index)\n",
    "    rescored = [{l: round(v) for l, v in d.items()} for d in rescored]\n",
    "    rescored = list(filter(lambda x: len(x[0].keys()) > 0 and x[1] > mincount, zip(rescored, doc_count)))\n",
    "    rescored.sort(key = lambda x: x[1], reverse=True)\n",
    "    rescored = list(map(lambda x: x[0], rescored))\n",
    "\n",
    "    cluster_dist = list(filter(lambda x: x[1] > mincount, zip(mgp.cluster_word_distribution, doc_count)))\n",
    "    cluster_dist.sort(key = lambda x: x[1], reverse=True)\n",
    "    cluster_dist = list(map(lambda x: x[0], cluster_dist))\n",
    "\n",
    "    display(rescored[clust], 20)\n",
    "    display(cluster_dist[clust], 20)\n",
    "    \n",
    "ClusterBarCharts(mgp, top_index, clust = 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convertForViz(model, documents, mincount = 15):\n",
    "    # This function converts the document from GSDMM encoding into LDA encoding so in can be visualized by pyLDAvis\n",
    "    vocab = vocabCreater(documents)\n",
    "    dictionary = tools.CreateVocab(documents)\n",
    "    rescored = tf_idf_creator(mgp.cluster_word_distribution, top_index)\n",
    "    # topic_term_dists\n",
    "    # Foreach topic what is the distribution for each word\n",
    "    topic_term_dists = []\n",
    "    for topic, c in zip(mgp.cluster_word_distribution, mgp.cluster_doc_count):\n",
    "        if c <= mincount: continue\n",
    "            \n",
    "        baseVec = np.zeros(len(dictionary.items()))\n",
    "\n",
    "        total = sum(np.array(list(topic.items()))[:,1].astype(int))\n",
    "        for word, count in topic.items():\n",
    "            prop = count/total#/vocab[word]\n",
    "            baseVec[dictionary[word]] = prop\n",
    "        topic_term_dists.append(baseVec)\n",
    "    \n",
    "    # doc_topic_dists\n",
    "    # Foreach document what is the probability to be in each of the different topics\n",
    "    doc_topic_dists = [mgp.score(doc) for doc in documents] # We score for all topics\n",
    "    # Here we remove topics that are too small to be considered\n",
    "    doc_topic_dists = list(map(lambda x: np.array(list(filter(lambda y: y[1] > mincount, zip(x, mgp.cluster_doc_count))))[:,0], doc_topic_dists))\n",
    "    # Reweight the remaining topics\n",
    "    doc_topic_dists = list(map(lambda x: x/sum(x), doc_topic_dists))\n",
    "    \n",
    "    doc_lengths = [len(doc) for doc in documents]\n",
    "\n",
    "    \n",
    "    data = {'topic_term_dists': topic_term_dists, \n",
    "            'doc_topic_dists': doc_topic_dists,\n",
    "            'doc_lengths': doc_lengths,\n",
    "            'vocab': [l for l, v in dictionary.items()],\n",
    "            'term_frequency': [vocab[l] for l, v in dictionary.items()]}\n",
    "    return pyLDAvis.display(pyLDAvis.prepare(**data))\n",
    "\n",
    "\n",
    "convertForViz(mgp, cleaned, mincount = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
