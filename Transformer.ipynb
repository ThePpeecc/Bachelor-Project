{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-YBGyxpZtYqL"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "#!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ItA09SjtHMq"
   },
   "outputs": [],
   "source": [
    "### This File is made to work on google colab, but can work with modifications locally\n",
    "\n",
    "# Transformer networks\n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertForPreTraining\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "### Load Numpy and Matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Metrics\n",
    "import sklearn.metrics as mec\n",
    "from mlxtend.evaluate import confusion_matrix \n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import collections as matcoll\n",
    "from graphviz import Digraph\n",
    "\n",
    "# Data handling\n",
    "from DataProcessor import DataInstance, DataProcesser\n",
    "\n",
    "# Utils\n",
    "import itertools\n",
    "import time\n",
    "import pickle\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "# Printing Options\n",
    "torch.set_printoptions(precision=5) # Printing accuracy\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "from google.colab import files # Colab specific\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GN1jxSRqor9j"
   },
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "NumberOfLabels = 18 # Number of participants\n",
    "BatchSize = 20\n",
    "NumberOfEpochs = 9 # 9 epochs with this setup seems to be the perfect balance between valid and train set\n",
    "TrainingSplit = .5 # Split between training and validation\n",
    "seed = 0\n",
    "lr = 0.00001 # Lerning Rate\n",
    "ModelDirectory = 'TurkuNLP/wikibert-base-da-cased'\n",
    "DataFile = './processed-set-final.bin'\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z7_-29FTgjmr"
   },
   "outputs": [],
   "source": [
    "ProcessedSet = DataProcesser.load(DataFile) # Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bBvQ2M2GjDoe"
   },
   "outputs": [],
   "source": [
    "for inst in ProcessedSet.instances:\n",
    "    inst.preds = torch.tensor([]) # Standardizing the non existing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zG-f8_RDov18"
   },
   "outputs": [],
   "source": [
    "def NavieSampler(Processor):\n",
    "    ros = RandomOverSampler(random_state=seed)\n",
    "\n",
    "    x = list(map(lambda x: [x], Processor.instances))\n",
    "    y = Processor.labels()\n",
    "\n",
    "    x, y = ros.fit_resample(x, y)\n",
    "\n",
    "    return DataProcesser.FromInstances(list(map(lambda v: v[0], x)))\n",
    "\n",
    "\n",
    "def Splitter(Processor, split = .2):\n",
    "    # Processor contains all data\n",
    "    # Split is the split ration\n",
    "    np.random.seed(seed)\n",
    "    data = Processor.instances\n",
    "\n",
    "    splitIndex = np.random.choice(len(data), len(data), replace=False).astype(int)\n",
    "\n",
    "\n",
    "    split = round((1.0-split)*len(data))\n",
    "\n",
    "    set1 = np.array(data)[splitIndex[0:split]]\n",
    "    set2 = np.array(data)[splitIndex[split:]]\n",
    "\n",
    "    return DataProcesser.FromInstances(set1),  DataProcesser.FromInstances(set2)\n",
    "\n",
    "\n",
    "Train, Valid = Splitter(ProcessedSet, TrainingSplit)\n",
    "\n",
    "Train = NavieSampler(Train)\n",
    "Train.cuda()\n",
    "Valid.cuda()\n",
    "ProcessedSet.cuda()\n",
    "\n",
    "\n",
    "TrainingSet = TensorDataset(Train.ids(), Train.types(), Train.masks(), Train.labels())\n",
    "\n",
    "# Final split\n",
    "print(len(ProcessedSet.instances))\n",
    "print(len(Train.instances))\n",
    "print(len(Valid.instances))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cnXSOC_wDhr4"
   },
   "outputs": [],
   "source": [
    "class MultiLableModel(BertForPreTraining):\n",
    "    def __init__(self, config, numLabels=2):\n",
    "        super(MultiLableModel, self).__init__(config)\n",
    "\n",
    "        self.numLabels = numLabels\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        self.classifier = nn.Linear(config.hidden_size, numLabels)        \n",
    "        self.drop = nn.Dropout(0.5)\n",
    "        self.out = nn.Sigmoid()\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None):\n",
    "        _, x = self.bert(input_ids, attention_mask, token_type_ids)\n",
    "        x = self.drop(x)\n",
    "        x = self.classifier(x)    \n",
    "        x = self.out(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Accuracy osv messurements\n",
    "def acc(Processor, model):\n",
    "    with torch.no_grad():\n",
    "        preds = model(Processor.ids(), Processor.masks(), Processor.types())\n",
    "        if preds.is_cuda:\n",
    "            preds = preds.cpu()\n",
    "\n",
    "        return preds.numpy()\n",
    "\n",
    "# Evaluates the model on a data processer set\n",
    "def eval(Processor, model):\n",
    "    preds = acc(Processor, model)\n",
    "    rounded = (preds/preds.max(axis=0)).round()\n",
    "    labels = Processor.labels().cpu()\n",
    "\n",
    "    return mec.classification_report(labels,rounded)\n",
    "\n",
    "\n",
    "def plotTrainingSession(loss_train, loss_valid):\n",
    "    plt.figure(figsize = (10,5))\n",
    "    plt.plot(loss_train, label='Training loss')\n",
    "    plt.plot(loss_valid, label='Validation loss')\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.ylabel('BCE Batch Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def ConfusionPlot(Processed, model):\n",
    "    preds = acc(Processed, model)\n",
    "    rounded = (preds/preds.max(axis=0)).round()\n",
    "\n",
    "    Map = np.zeros((18,18))\n",
    "    labels = Processed.labels().cpu()\n",
    "\n",
    "    for x, y in zip(rounded, labels):\n",
    "    label = np.where(y==1.0)[0][0]\n",
    "    Map[label] += x\n",
    "    Map[label] = Map[label]\n",
    "\n",
    "    for i, lane in enumerate(Map):\n",
    "    Map[i] = Map[i]/sum(lane)\n",
    "\n",
    "\n",
    "    plt.figure(figsize = (20,20))\n",
    "    classes = range(18)\n",
    "    plt.imshow(Map.T, interpolation='nearest')\n",
    "    plt.clim(0.0,1.0)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "print(\"Done\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80UvlvufYrDY"
   },
   "outputs": [],
   "source": [
    "# Loading of the model\n",
    "torch.manual_seed(seed) # set fixed random seed for reproducibility\n",
    "\n",
    "print(\"Initializeing Model\")\n",
    "model = MultiLableModel.from_pretrained(ModelDirectory, numLabels = NumberOfLabels).cuda()\n",
    "print(\"Model Loaded\")\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(TrainingSet,\n",
    "    batch_size=BatchSize, shuffle=True)\n",
    "\n",
    "trainLossEpochs = []\n",
    "validLossEpochs = []\n",
    "\n",
    "print(\"Starting training\")\n",
    "for epoch in range(1, NumberOfEpochs + 1):\n",
    "    model.train()\n",
    "    totalBatchLoss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        ids, types, masks, labels = data\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(ids, masks, types)\n",
    "        \n",
    "        loss = loss_function(outputs, labels.float())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        totalBatchLoss += loss.item()\n",
    "\n",
    "   \n",
    "    print('====> Epoch: {} Total Loss: {:.4f} Average batch Loss: {:.4f}\\r'.format(\n",
    "          epoch, totalBatchLoss, totalBatchLoss/batch_idx))\n",
    "    \n",
    "    trainLossEpochs.append(totalBatchLoss/batch_idx)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      out = model(Valid.ids(), Valid.masks(), Valid.types())\n",
    "      labels = Valid.labels().float()\n",
    "\n",
    "      validLoss = loss_function(out, labels).item()\n",
    "      validLossEpochs.append(validLoss)\n",
    "      print('===> Validation Loss {}'.format(validLoss))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r3oYUa8jxDUB"
   },
   "outputs": [],
   "source": [
    "print(\"Loss plot\")\n",
    "plotTrainingSession(trainLossEpochs, validLossEpochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMwKHeqYcTOU"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./Temp\") # If gpu memory is too little, save the model and then restart the notebook, and then load it back in the next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9JJTV9excd3V"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(seed) # set fixed random seed for reproducibility\n",
    "print(\"Initializeing Model\")\n",
    "model = MultiLableModel.from_pretrained(ModelDirectory, numLabels = NumberOfLabels).cuda()\n",
    "print(\"Model Loaded\")\n",
    "\n",
    "model.load_state_dict(torch.load(\"./Temp\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GxldKeF55Y1h"
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4sOoZ6kiwiw_"
   },
   "outputs": [],
   "source": [
    "print(\"Training\")\n",
    "evalText = eval(Train, model)\n",
    "print(evalText)\n",
    "print(\"--\"*20)\n",
    "print(\"Validation\")\n",
    "evalText = eval(Valid, model)\n",
    "print(evalText)\n",
    "print(\"--\"*20)\n",
    "print(\"Over All\")\n",
    "evalText = eval(ProcessedSet, model)\n",
    "print(evalText)\n",
    "print(\"--\"*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U4cZEUzlcb76"
   },
   "outputs": [],
   "source": [
    "print(\"Over all\")\n",
    "ConfusionPlot(ProcessedSet, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "40fw5xZwd5wz"
   },
   "outputs": [],
   "source": [
    "print(\"Training\")\n",
    "ConfusionPlot(Train, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1QrGNq1kd74q"
   },
   "outputs": [],
   "source": [
    "print(\"Validation\")\n",
    "ConfusionPlot(Valid, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gFi-fDg1m8e9"
   },
   "outputs": [],
   "source": [
    "def lineScatter(x, y, fileName = None):\n",
    "    lines = []\n",
    "    for i in range(len(x)):\n",
    "        pair=[(x[i],0), (x[i], y[i])]\n",
    "        lines.append(pair)\n",
    "        \n",
    "    linecoll = matcoll.LineCollection(lines)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.add_collection(linecoll)\n",
    "\n",
    "    plt.scatter(x,y)\n",
    "    plt.xlabel(\"Participants\")\n",
    "    plt.ylabel(\"Sum probability\")\n",
    "    plt.xticks(x)\n",
    "    plt.ylim(0,1)\n",
    "\n",
    "    plt.show()\n",
    "    if fileName != None:\n",
    "        plt.draw()\n",
    "        fig.savefig(fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yDt5rUtBnAT7"
   },
   "outputs": [],
   "source": [
    "Map = np.zeros((18,18))\n",
    "\n",
    "preds = acc(ProcessedSet, model)\n",
    "labels = ProcessedSet.labels().cpu()\n",
    "\n",
    "for x, y in zip(preds, labels):\n",
    "    label = np.where(y==1.0)[0][0]\n",
    "    Map[label] += x\n",
    "    Map[label] = Map[label]\n",
    "\n",
    "for i, lane in enumerate(Map):\n",
    "    Map[i] = Map[i]/sum(lane)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q60Wm_MH3_Cw"
   },
   "outputs": [],
   "source": [
    "!mkdir out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gVu9Qt9bn1PB"
   },
   "outputs": [],
   "source": [
    "for i, dist in enumerate(Map):\n",
    "    print(\"Particitpant {}\".format(i))\n",
    "    lineScatter(range(18), dist/sum(dist), \"./out/par_{}_dist\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wOIIMkY71uc6"
   },
   "outputs": [],
   "source": [
    "!zip -r ./file.zip ./out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dz79vM-OoEst"
   },
   "outputs": [],
   "source": [
    "files.download(\"./file.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iL_l8byv3QDe"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
