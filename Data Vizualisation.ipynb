{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader\n",
    "import loader \n",
    "\n",
    "# Import numpy and pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import Plotting tools\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib import collections as matcoll\n",
    "\n",
    "# Utils\n",
    "import tools\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load data in raw\n",
    "raw = loader.LoadRaw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hist(data, title = ''):\n",
    "    return pd.DataFrame({title: data}).hist(grid = False, bins = 15)\n",
    "\n",
    "def box(data):\n",
    "    return pd.DataFrame(data).boxplot(grid = False, figsize = (5, 9))\n",
    "\n",
    "def printStats(data):\n",
    "    print(\"Number of documents {}\".format(len(data)))\n",
    "    print(\"Number of empyty documents {}\".format(sum([1 if len(d) == 0 else 0 for d in data])))\n",
    "\n",
    "    Llist1 = list(map(len, data))\n",
    "    print(\"Longest document {}\".format(max(Llist1)))\n",
    "    print(\"Average length of document {}\".format(sum(Llist1)/len(data)))\n",
    "    \n",
    "    print(\"Number of tokens {}\".format(sum(Llist1)))\n",
    "    vocab = tools.vocabCreater(data)\n",
    "    print(\"Number of unique tokens {}\".format(len(vocab)))\n",
    "    \n",
    "    Llist2 = list(map(len, vocab))\n",
    "    print(\"Longest token {}\".format(max(Llist2)))\n",
    "    print(\"Average token length {}\".format(sum(Llist2)/len(vocab)))\n",
    "    \n",
    "    hist(Llist1, 'Histeogram over document lengths')\n",
    "    hist(Llist2, 'Histeogram over token lengths')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    box({'Document Lengths (Shaken)': Llist1 + + np.random.uniform(-1,1,len(Llist1))})\n",
    "    plt.title( '' )\n",
    "    \n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    np.random.normal(0,1,100)\n",
    "    box({'Token Lengths (shaken)': Llist2 + np.random.uniform(-1,1,len(Llist2))})\n",
    "    \n",
    "    plt.title( '' )\n",
    "    plt.show()\n",
    "    \n",
    "def PipeLineWords(data):\n",
    "    tokens = tools.tokenize(data)\n",
    "    stemmed = tools.stemDocument(tokens)\n",
    "    added = tools.getMoreTokens(stemmed, 2)\n",
    "    cleaned = tools.cleanDoc(added, 3, 50)\n",
    "    return tokens, stemmed, added, cleaned\n",
    "\n",
    "def cleanPersonData(data):\n",
    "    processed = []\n",
    "    for person in data:\n",
    "        tokens, stemmed, added, cleaned = PipeLineWords(person)\n",
    "        raw = tools.rawTokenize(person)\n",
    "        processed.append(raw)\n",
    "    return processed\n",
    "\n",
    "def lineScatter(x, y):\n",
    "    lines = []\n",
    "    for i in range(len(x)):\n",
    "        pair=[(x[i],0), (x[i], y[i])]\n",
    "        lines.append(pair)\n",
    "        \n",
    "    linecoll = matcoll.LineCollection(lines)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.add_collection(linecoll)\n",
    "\n",
    "    plt.scatter(x,y)\n",
    "\n",
    "    plt.xticks(x)\n",
    "    plt.ylim(0,max(y) + min(y))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def personalStats(data):\n",
    "    cleaned = cleanPersonData(data)\n",
    "    tokens = list(map(sum, [[len(sent) for sent in participant] for participant in cleaned]))\n",
    "    y = list(map(len, cleaned))\n",
    "    x = range(len(data))\n",
    "    \n",
    "    avg = sum(y)/len(x)\n",
    "    \n",
    "    lineScatter(x, y)\n",
    "    lineScatter(x, tokens)\n",
    "        \n",
    "    print(\"-\"*20)\n",
    "    print(\"Average number of sentences for each participant\")\n",
    "    print(sum(y)/len(data))\n",
    "    print(\"Persentage of the 3 most active participants sentence split\")\n",
    "    print((y[11] + y[12] + y[13])/sum(y))\n",
    "    print(\"Persentage of the 3 most active participants tokens split\")\n",
    "    print((tokens[11] + tokens[13] + tokens[3])/sum(tokens))\n",
    "    print(\"Total Sum of tokens\")\n",
    "    print(sum(tokens))\n",
    "    print(\"-\"*20)\n",
    "    \n",
    "def display(vocab, n = 5, start = 0, title = \"\"):\n",
    "    \n",
    "    vocab = sorted(vocab.items(), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    labels = np.array(vocab)[start:start+n,0]\n",
    "    counts = np.array(vocab)[start:start+n,1].astype(int)\n",
    "\n",
    "    bar_width = 0.35\n",
    "\n",
    "    indexes = np.arange(len(labels))\n",
    "    fig = plt.figure()\n",
    "    plt.figure(figsize=(20,10))\n",
    "\n",
    "    plt.bar(indexes, counts)\n",
    "\n",
    "    # add labels\n",
    "    plt.xticks(indexes + bar_width, labels)\n",
    "    plt.ylabel('Count', fontsize=16)    \n",
    "    plt.title(title, fontsize=20)\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def top_words(dis, count, index, num = 5, start = 0):\n",
    "\n",
    "    for i in index:\n",
    "        s = sorted(dis[i].items(), key=lambda x: x[1], reverse=True)\n",
    "        print(\"Cluster {} with {} documents : {}\".format(i, count[i], s[start:start+num]))\n",
    "    \n",
    "    \n",
    "def formatTFIDFString(clust):\n",
    "\n",
    "    formatted = []\n",
    "    for word, value in clust:\n",
    "        formatted.append(\"{}: {:.1f}\".format(word, value))\n",
    "        \n",
    "    return formatted\n",
    "\n",
    "\n",
    "def tf_idf_creator(dis, index):\n",
    "    # First we build a dictionary to look up the count of the words in each cluster\n",
    "    cross_vocab = {}\n",
    "\n",
    "    for i, clus in enumerate(dis):\n",
    "        for word, count in clus.items():\n",
    "            if word not in cross_vocab:\n",
    "                cross_vocab[word] = np.zeros(len(index))\n",
    "                cross_vocab[word][i] += 1\n",
    "            else:\n",
    "                cross_vocab[word][i] += 1\n",
    "\n",
    "    rescored = [{} for _ in range(len(index))]\n",
    "    for i in index:\n",
    "        newDict = {}\n",
    "        for word, count in dis[i].items():\n",
    "            newDict[word] = count * math.log(len(index)/sum(cross_vocab[word]))\n",
    "        rescored[i] = newDict\n",
    "        \n",
    "    return rescored\n",
    "    \n",
    "    \n",
    "# Simple tf-idf sorter\n",
    "def top_tf_idf(dis, doc_count, index, num = 5, start = 0):\n",
    "    \n",
    "    rescored = tf_idf_creator(dis, index)\n",
    "    for i in index:\n",
    "        s = sorted(rescored[i].items(), key=lambda x: x[1], reverse=True)\n",
    "        print(\"Cluster {} with {} documents : {}\".format(i, doc_count[i], formatTFIDFString(s[start:start+num])))\n",
    "\n",
    "        \n",
    "def doc_tabel(data):\n",
    "    tokens, stemmed, added, cleaned = PipeLineWords(data)\n",
    "    \n",
    "    stemmed_vocab = tools.vocabCreater(added)\n",
    "    newAr = []    \n",
    "    vocab_count = {l: v for l, v in stemmed_vocab.items()} \n",
    "    \n",
    "    for i, sent in enumerate(stemmed):\n",
    "        nSent = tools.cleanSent(tools.getMoreTokens([sent])[0], vocab_count)\n",
    "        newAr.append([data[i], nSent])\n",
    "            \n",
    "    df = pd.DataFrame(newAr)\n",
    "    df.columns =[\"Sentence\", \"Cleaned\"]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = tools.getMoreSent(raw)\n",
    "tokens, stemmed, added, cleaned = PipeLineWords(data)\n",
    "rawTokens = tools.rawTokenize(data)\n",
    "cleanedVocab = sorted(tools.vocabCreater(cleaned).items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Data Stats\")\n",
    "print(\"**\"*24)\n",
    "print(\"Total chars {}\".format(len(raw)))\n",
    "\n",
    "print(\"{} {} {}\".format(\"*\"*18, \"Raw Tokens\", \"*\"*18))\n",
    "printStats(rawTokens)\n",
    "print(\"{} {} {}\".format(\"*\"*20, \"Tokens\", \"*\"*20))\n",
    "printStats(tokens)\n",
    "print(\"{} {} {}\".format(\"*\"*19, \"Stemmed\", \"*\"*20))\n",
    "printStats(stemmed)\n",
    "print(\"{} {} {}\".format(\"*\"*17, \"Added Tokens\", \"*\"*17))\n",
    "printStats(added)\n",
    "print(\"{} {} {}\".format(\"*\"*19, \"Cleaned\", \"*\"*20))\n",
    "printStats(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "personData = loader.loadDataParticipants()\n",
    "personalStats(personData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = tools.stemmedReverse(tokens)\n",
    "ngramVocab = tools.vocabCreater(tools.getNTokens(rawTokens))\n",
    "vocab = tools.vocabCreater(tokens)\n",
    "stemmed_vocab = tools.vocabCreater(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(vocab, 20, title = \"Most commen words\")\n",
    "display(stemmed_vocab, 20, title = \"Most common stemmed words\")\n",
    "display(ngramVocab, 20, title=\"Most common bi-grams with stopwords\")\n",
    "display(tools.vocabCreater(tools.getNTokens(stemmed)), 20, title=\"Most common bi-grams without stopwords\")\n",
    "display(tools.vocabCreater(rawTokens), 20, title=\"Most common stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.options.display.max_rows = 100\n",
    "doc_tabel(data).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
